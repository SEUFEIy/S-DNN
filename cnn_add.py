import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class DynamicCNN(nn.Module):
    def __init__(self, input_channels, conv_hidden_layers, num_classes, size=32, device='cuda'):
        super(DynamicCNN, self).__init__()
        self.layers = nn.ModuleList()
        self.num_class = num_classes
        self.device = torch.device(device=device)
        self.input_channels = input_channels
        self.hidden_sizes = conv_hidden_layers.copy()  # å…³é”®ä¿®å¤ï¼šä¿å­˜ä¸ºå®ä¾‹å±æ€§
        self.size = size
        self.ewc_params = {}       # å­˜å‚¨å„ä»»åŠ¡æœ€ä¼˜å‚æ•° {task_id: {param_name: tensor}}
        self.fisher = {}           # å­˜å‚¨å„ä»»åŠ¡Fisherä¿¡æ¯ {task_id: {param_name: tensor}}
        self.plasticity_mask = {}  # å¯å¡‘æ€§æ©ç  {param_name: mask}
        self.task_count = 0        # å·²å­¦ä¹ ä»»åŠ¡è®¡æ•°å™¨
        
        # è‡ªåŠ¨ç”Ÿæˆæ¯éš”ä¸¤å±‚çš„æ®‹å·®å±‚ç´¢å¼•ï¼ˆå¦‚ç¬¬2ã€5ã€8å±‚ï¼‰
        self.residual_layers = [i for i in range(len(self.hidden_sizes)) if (i + 1) % 3 == 0]  # ä½¿ç”¨self.hidden_sizes
        
        # åˆå§‹åŒ–æ± åŒ–ã€æ¿€æ´»ã€å±•å¹³
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2).to(self.device)
        self.act = nn.ReLU().to(self.device)
        self.flatten = nn.Flatten().to(self.device)

        # åˆå§‹åŒ–å·ç§¯å±‚ï¼ˆè‡ªåŠ¨æ·»åŠ æ®‹å·®è·¯å¾„ï¼‰
        in_ch = self.input_channels
        for layer_idx, out_ch in enumerate(self.hidden_sizes):  # ä½¿ç”¨self.hidden_sizes
            current_layer = nn.ModuleList()
            
            # ä¸»è·¯å¾„ï¼šConv + BN
            current_layer.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1).to(self.device))
            current_layer.append(nn.BatchNorm2d(out_ch).to(self.device))
            
            # å¦‚æœå½“å‰å±‚éœ€è¦æ®‹å·®è¿æ¥ï¼Œæ·»åŠ 1x1å·ç§¯
            if layer_idx in self.residual_layers:
                residual_conv = nn.Conv2d(in_ch, out_ch, kernel_size=1).to(self.device)
                nn.init.kaiming_normal_(residual_conv.weight, mode='fan_out', nonlinearity='relu')
                current_layer.append(residual_conv)
            
            self.layers.append(current_layer)
            in_ch = out_ch
        
        self._update_fc_layer()

    def _update_residual_layers(self):
        """æ›´æ–°æ®‹å·®å±‚ç´¢å¼•"""
        self.residual_layers = [i for i in range(len(self.hidden_sizes)) if (i + 1) % 3 == 0]
    
    def compute_fisher(self, dataloader):
        """è®¡ç®—å½“å‰ä»»åŠ¡çš„Fisherä¿¡æ¯çŸ©é˜µ"""
        self.eval()
        fisher_dict = {n: torch.zeros_like(p) for n, p in self.named_parameters() if p.requires_grad}
        
        for inputs, _ in dataloader:
            inputs = inputs.to(self.device)
            self.zero_grad()
            outputs = self(inputs)
            loss = F.cross_entropy(outputs, outputs.argmax(dim=1))  # æ— ç›‘ç£ä»£ç†æŸå¤±
            loss.backward()
            
            for n, p in self.named_parameters():
                if p.grad is not None:
                    fisher_dict[n] += p.grad.detach().clone() ** 2
        
        # å½’ä¸€åŒ–å¹¶å­˜å‚¨
        self.fisher[self.task_count] = {n: f / len(dataloader) for n, f in fisher_dict.items()}
        self.ewc_params[self.task_count] = {n: p.detach().clone() for n, p in self.named_parameters()}

    def ewc_loss(self):
        """è®¡ç®—æ‰€æœ‰å·²å­¦ä»»åŠ¡çš„EWCæ­£åˆ™åŒ–æŸå¤±"""
        loss = 0
        for task in range(self.task_count):
            for n, p in self.named_parameters():
                if n in self.fisher[task] and n in self.ewc_params[task]:
                    loss += (self.fisher[task][n] * 
                            (p - self.ewc_params[task][n]) ** 2).sum()
        return loss

    def update_plasticity_mask(self, decay_factor=0.3):
        """æ ¹æ®å‚æ•°å˜åŒ–å¹…åº¦æ›´æ–°æ¢¯åº¦æ©ç """
        for n, p in self.named_parameters():
            if n not in self.plasticity_mask:
                self.plasticity_mask[n] = torch.ones_like(p)
            
            # è®¡ç®—å‚æ•°å˜åŒ–é‡ï¼ˆç›¸å¯¹äºåˆå§‹å€¼ï¼‰
            delta = torch.abs(p.detach() - self.ewc_params.get(0, {}).get(n, p.detach()))
            self.plasticity_mask[n] = torch.exp(-decay_factor * delta)

    def apply_gradient_mask(self):
        """åœ¨åå‘ä¼ æ’­ååº”ç”¨å¯å¡‘æ€§æ©ç """
        for n, p in self.named_parameters():
            if p.grad is not None and n in self.plasticity_mask:
                p.grad *= self.plasticity_mask[n]
                
    def _update_fc_layer(self):
        """åŠ¨æ€è®¡ç®—å…¨è¿æ¥å±‚è¾“å…¥ç»´åº¦ï¼ˆä¸¥æ ¼å¯¹é½forwardæµç¨‹ï¼‰"""
        with torch.no_grad():
            dummy_input = torch.randn(1, self.input_channels, self.size, self.size).to(self.device)
            for layer_idx, layer_group in enumerate(self.layers):
                identity = dummy_input.clone()
                
                # ä¸»è·¯å¾„ï¼šConv -> BN
                x = layer_group[0](dummy_input)
                x = layer_group[1](x)
                
                # æ®‹å·®è·¯å¾„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if layer_idx in self.residual_layers:
                    residual = layer_group[2](identity)
                    x += residual  # æ®‹å·®ç›¸åŠ 
                
                # æ¿€æ´» + æ± åŒ–
                x = self.act(x)
                x = self.pool(x)
                dummy_input = x
            
            in_features = dummy_input.numel() // dummy_input.shape[0]
            self.fc = nn.Linear(in_features, self.num_class).to(self.device)

    def forward(self, x):
        for layer_idx, layer_group in enumerate(self.layers):
            identity = x.clone()
            
            # ä¸»è·¯å¾„ï¼šConv -> BN
            x = layer_group[0](x)
            x = layer_group[1](x)
            
            # æ®‹å·®è·¯å¾„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if layer_idx in self.residual_layers:
                residual = layer_group[2](identity)
                x += residual  # æ®‹å·®ç›¸åŠ 
            
            # æ¿€æ´» + æ± åŒ–
            x = self.act(x)
            x = self.pool(x)
        
        x = self.flatten(x)
        x = self.fc(x)
        return x

    def modify_channels(self, layer_index, increase_channel):
        """ä¿®æ”¹æŒ‡å®šå±‚é€šé“æ•°ï¼ˆåŒæ­¥æ›´æ–°æ®‹å·®è·¯å¾„ï¼‰"""
        assert 0 <= layer_index < len(self.layers), "Invalid layer index"
        
        # ä¿®æ”¹ä¸»å·ç§¯å±‚
        main_conv = self.layers[layer_index][0]
        new_main_conv = nn.Conv2d(
            main_conv.in_channels,
            main_conv.out_channels + increase_channel,
            kernel_size=3, 
            padding=1
        ).to(self.device)
        new_main_conv.weight.data[:main_conv.out_channels] = main_conv.weight.data
        self.layers[layer_index][0] = new_main_conv
        
        # æ›´æ–°BNå±‚
        new_bn = nn.BatchNorm2d(new_main_conv.out_channels).to(self.device)
        self.layers[layer_index][1] = new_bn
        
        # å¦‚æœè¯¥å±‚æœ‰æ®‹å·®è¿æ¥ï¼Œæ›´æ–°æ®‹å·®å·ç§¯
        if layer_index in self.residual_layers:
            residual_conv = self.layers[layer_index][2]
            new_residual_conv = nn.Conv2d(
                residual_conv.in_channels,
                new_main_conv.out_channels,  # è¾“å‡ºé€šé“ä¸ä¸»è·¯å¾„åŒæ­¥
                kernel_size=1
            ).to(self.device)
            new_residual_conv.weight.data[:residual_conv.out_channels] = residual_conv.weight.data
            self.layers[layer_index][2] = new_residual_conv
        
        # æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥é€šé“
        if layer_index + 1 < len(self.layers):
            next_conv = self.layers[layer_index + 1][0]
            new_next_conv = nn.Conv2d(
                new_main_conv.out_channels,
                next_conv.out_channels,
                kernel_size=3,
                padding=1
            ).to(self.device)
            new_next_conv.weight.data[:, :next_conv.in_channels] = next_conv.weight.data
            self.layers[layer_index + 1][0] = new_next_conv
        
        self._update_fc_layer()  # ä¿®æ”¹åæ›´æ–°å…¨è¿æ¥å±‚
        self._update_residual_layers()  # ä¿®æ”¹åæ›´æ–°ç´¢å¼•

    # -------------------------- å…³é”®ä¿®æ”¹2ï¼šä¿®æ­£åˆå¹¶å­å±‚é€»è¾‘ --------------------------
    def merge_sub_layers(self, layer_index):
        if len(self.layers[layer_index]) == 2:
            return
        
        # è·å–ä¸¤ä¸ªå­å·ç§¯å±‚å’Œå¯¹åº”çš„BNå±‚
        conv1 = self.layers[layer_index][0]
        conv2 = self.layers[layer_index][1]
        bn = self.layers[layer_index][2]
        
        # åˆå¹¶åçš„è¾“å…¥é€šé“æ˜¯conv1.in_channelsï¼Œè¾“å‡ºæ˜¯conv1.out_channels + conv2.out_channels
        merge_conv = nn.Conv2d(conv1.in_channels, conv1.out_channels + conv2.out_channels, 
                              kernel_size=3, padding=1).to(self.device)
        merge_conv.weight.data[:conv1.out_channels] = conv1.weight.data
        merge_conv.weight.data[conv1.out_channels:] = conv2.weight.data
        
        # åˆå¹¶BNå±‚å‚æ•°ï¼ˆå‡è®¾åŸBNå±‚å·²ç»å¤„ç†äº†åˆå¹¶åçš„é€šé“ï¼‰
        self.layers[layer_index] = nn.ModuleList([merge_conv, bn])
        self._update_fc_layer()  # åˆå¹¶åæ›´æ–°å…¨è¿æ¥å±‚
        self._update_residual_layers()  # ä¿®æ”¹åæ›´æ–°ç´¢å¼•

    def add_conv_layer(self, new_out_channels):
        prev_layer = self.layers[-1][0]
        new_conv = nn.Conv2d(prev_layer.out_channels, new_out_channels, 
                            kernel_size=3, padding=1).to(self.device)
        new_bn = nn.BatchNorm2d(new_out_channels).to(self.device)
        self.layers.append(nn.ModuleList([new_conv, new_bn]))
        self._update_fc_layer()  # æ·»åŠ å±‚åæ›´æ–°å…¨è¿æ¥å±‚
        self._update_residual_layers()  # ä¿®æ”¹åæ›´æ–°ç´¢å¼•


def train(model, optimizer, epochs=2, ewc_lambda=1e4):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        # å‡è®¾train_loaderæ˜¯å½“å‰ä»»åŠ¡çš„æ•°æ®åŠ è½½å™¨
        for x, y in train_loader:  # Ensure train_loader is defined before this loop
            x, y = x.to(model.device), y.to(model.device)
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(x)
            loss = F.cross_entropy(outputs, y)
            
            # ğŸš€ æ·»åŠ EWCæ­£åˆ™é¡¹ï¼ˆæ’é™¤å½“å‰ä»»åŠ¡ï¼‰
            if model.task_count > 0:
                loss += ewc_lambda * model.ewc_loss()
            
            # åå‘ä¼ æ’­ä¸æ¢¯åº¦æ©ç 
            loss.backward()
            model.apply_gradient_mask()  # ğŸš€ åº”ç”¨æ©ç 
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')
    
    # ğŸš€ ä»»åŠ¡ç»“æŸåæ›´æ–°Fisherä¿¡æ¯
    model.compute_fisher(train_loader)
    model.task_count += 1

if __name__ == '__main__':
    input_channels = 3  # è¾“å…¥ä¸º3é€šé“å›¾åƒ
    in_size = 32
    num_class = 10
    conv_layers = [16]  # ç°æœ‰å·ç§¯å±‚
    bs = 16

    net = DynamicCNN(input_channels=input_channels, conv_hidden_layers=conv_layers, num_classes=num_class).cuda()

    # Define train_loader with a dataset (e.g., CIFAR-10)
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)

    # ä½¿ç”¨ç¤ºä¾‹
    x = torch.randn(bs, input_channels, in_size, in_size).cuda()  # å‡è®¾è¾“å…¥ä¸º5ä¸ª28x28çš„å›¾ç‰‡
    y = torch.randint(0, num_class, (bs,)).cuda()

    # è®­ç»ƒè®¾ç½®
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = optim.SGD(net.parameters(), lr=0.01)

    print(net)
    train(optimizer=optimizer)

    net.modify_channels(layer_index=len(net.layers) - 1, increase_channel=24)
    print(net)
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    train(optimizer=optimizer)
    net.merge_sub_layers(layer_index=len(net.layers) - 1)
    print(net)

    net.modify_channels(layer_index=len(net.layers) - 1, increase_channel=24)
    print(net)
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    train(optimizer=optimizer)
    net.merge_sub_layers(layer_index=len(net.layers) - 1)
    print(net)

    # =========================== æ·»åŠ å·ç§¯å±‚ï¼Œå‡è®¾è¾“å‡ºé€šé“ä¸º24 ====================
    net.add_conv_layer(new_out_channels=25)
    print(net)
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    train(optimizer=optimizer)
    net.merge_sub_layers(layer_index=len(net.layers) - 1)
    print(net)
    
    net.modify_channels(layer_index=len(net.layers) - 1, increase_channel=24)
    print(net)
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    train(optimizer=optimizer)
    net.merge_sub_layers(layer_index=len(net.layers) - 1)
    print(net)

    # net.add_conv_layer(layer_index=2, new_out_channels=50)
    # print(net)
    # optimizer = optim.SGD(net.parameters(), lr=0.01)
    # train(optimizer=optimizer, epochs=5)
